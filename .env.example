# --- Docker Compose profiles (controls which services start) ---
# Base stack (no AI)  → leave empty or comment out
# AI + cloud LLM      → ai
# AI + local Ollama    → ai,ollama
COMPOSE_PROFILES=

# For GPU support, uncomment (requires NVIDIA Container Toolkit):
# COMPOSE_FILE=docker-compose.yml:docker-compose.gpu.yml

# MinIO object storage
MINIO_USER=clipfeed
MINIO_PASSWORD=changeme_strong_password_here

# JWT signing key -- generate with: openssl rand -base64 32
JWT_SECRET=changeme_generate_with_openssl

# Separate signing key for admin JWTs -- limits blast radius of a key compromise.
# If unset, falls back to JWT_SECRET (but you should set a distinct key).
ADMIN_JWT_SECRET=changeme_generate_with_openssl

# Admin credentials for the status dashboard
ADMIN_USERNAME=admin
ADMIN_PASSWORD=changeme_admin_password

# Worker secret for internal API auth -- generate with: openssl rand -base64 32
WORKER_SECRET=changeme_generate_with_openssl

# Storage management
STORAGE_LIMIT_GB=50
CLIP_TTL_DAYS=30

# Download and Processing Limits
# PROCESSING_MODE can be "transcode" (default, scales to 720p vertical) or "copy" (very fast, keeps original format)
PROCESSING_MODE=transcode
MAX_VIDEO_DURATION=3600
MAX_DOWNLOAD_SIZE_MB=2048
MIN_CLIP_SECONDS=15
MAX_CLIP_SECONDS=90
TARGET_CLIP_SECONDS=45

# Worker settings (tune to your NAS hardware)
MAX_WORKERS=4
WHISPER_MODEL=medium

# Score updater interval (seconds)
SCORE_UPDATE_INTERVAL=900

# LLM provider: ollama, openai, anthropic
LLM_PROVIDER=ollama
# Base URL (auto-detected for known providers; override for custom endpoints)
LLM_BASE_URL=
# API key (not needed for local Ollama)
LLM_API_KEY=
# Anthropic API version header (when LLM_PROVIDER=anthropic)
ANTHROPIC_VERSION=2023-06-01
# Model name (falls back to OLLAMA_MODEL for local Ollama)
LLM_MODEL=
# Default model when using local Ollama
OLLAMA_MODEL=llama3.2:3b
